<!-- <!doctype html>
<html lang="en">
 <head>
   <meta charset="UTF-8" />
   <meta name="viewport" content="width=device-width, initial-scale=1.0" />
   <title>Depth Map on Video</title>
   <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/depth-estimation@2.0.0/dist/index.js"></script>
   <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
   <style>
     body {
       margin: 0;
       display: flex;
       flex-direction: column;
       align-items: center;
       justify-content: center;
       background-color: black;
     }
     #video,
     #depthCanvas {
       position: absolute;
       top: 0;
       left: 0;
       width: 100%;
       height: auto;
     }
   </style>
 </head>
 <body>
   <video id="video" autoplay playsinline></video>
   <canvas id="depthCanvas"></canvas>

   <script>
     const video = document.getElementById("video");
     const depthCanvas = document.getElementById("depthCanvas");
     const depthCtx = depthCanvas.getContext("2d");
     let depthEstimator;
     async function toggleDepthMap() {
       if (depthCanvas.style.display === "none") {
         document.getElementById("toggleButton").innerText =
           "Скрыть карту глубины";
         depthCanvas.style.display = "block";
         if (!video) await startCamera();

         // Проверяем, загружена ли библиотека
         if (typeof depthEstimation !== "undefined") {
           if (!depthEstimator) await initializeDepthModel();
           updateDepthMap();
         } else {
           console.error(
             "depthEstimation не найден. Убедитесь, что библиотека правильно загружена.",
           );
         }
       } else {
         document.getElementById("toggleButton").innerText =
           "Показать карту глубины";
         depthCanvas.style.display = "none";
       }
     }
     // Запуск камеры
     async function startCamera() {
       const stream = await navigator.mediaDevices.getUserMedia({
         video: { facingMode: "environment" },
       });
       video.srcObject = stream;

       // Ожидаем загрузки видео
       video.onloadedmetadata = () => {
         video.play();
         depthCanvas.width = video.videoWidth;
         depthCanvas.height = video.videoHeight;
         initializeDepthModel();
       };
     }

     // Инициализация модели TensorFlow
     async function initializeDepthModel() {
       depthEstimator = await depthEstimation.createEstimator(
         depthEstimation.SupportedModels.COCODepthEstimation,
       );
       console.log("Depth model loaded");
       processVideo();
     }

     // Обработка видео и отрисовка карты глубины
     async function processVideo() {
       if (depthEstimator && video.readyState >= video.HAVE_ENOUGH_DATA) {
         // Генерация карты глубины
         const depthMap = await depthEstimator.estimateDepth(video);
         const depthData = depthMap.dataSync();
         const width = depthCanvas.width;
         const height = depthCanvas.height;

         // Подготовка пикселей для отрисовки
         const imageData = depthCtx.createImageData(width, height);
         for (let i = 0; i < depthData.length; i++) {
           const depthValue = Math.floor((1 - depthData[i]) * 255);
           imageData.data[i * 4] = depthValue; // Red
           imageData.data[i * 4 + 1] = depthValue; // Green
           imageData.data[i * 4 + 2] = depthValue; // Blue
           imageData.data[i * 4 + 3] = 150; // Alpha (прозрачность)
         }
         depthCtx.putImageData(imageData, 0, 0);
       }

       // Продолжаем обработку
       requestAnimationFrame(processVideo);
     }

     // Запуск приложения
     startCamera().catch((err) =>
       console.error("Error accessing camera:", err),
     );
     document.body.innerHTML += '<button id="toggleButton" onclick="toggleDepthMap()">Показать карту глубины</button>'
   </script>
 </body>
</html> -->

<!-- <!doctype html>
<html lang="en">
 <head>
   <meta charset="UTF-8" />
   <meta name="viewport" content="width=device-width, initial-scale=1.0" />
   <title>1 Depth Estimation with OpenCV.js</title>
   <script async src="https://cdn.jsdelivr.net/npm/opencv.js" onload="onOpenCvReady();" type="text/javascript"></script>
 </head>
 <body>
   <h1>Depth Estimation from Camera Stream</h1>
   <video id="video" width="640" height="480" autoplay></video>
   <canvas id="depthCanvas" width="640" height="480"></canvas>

   <script>
     let videoElement;
     let canvasElement;
     let canvasContext;
     let mat;
     let thresholdMat;

     // Функция, которая будет вызвана, когда OpenCV.js загрузится
     function onOpenCvReady() {
       if (cv && cv.getBuildInformation) {
         console.log("OpenCV.js загружен и готов к использованию.");
       } else {
         console.error("OpenCV.js не загружен или не готов.");
         return;
       }

       videoElement = document.getElementById("video");
       canvasElement = document.getElementById("depthCanvas");
       canvasContext = canvasElement.getContext("2d");

       // Запрашиваем доступ к камере
       navigator.mediaDevices
         .getUserMedia({ video: true })
         .then((stream) => {
           videoElement.srcObject = stream;
           videoElement.play();
         })
         .catch((err) => {
           console.error("Ошибка доступа к камере", err);
           alert(
             "Не удается получить доступ к камере. Убедитесь, что разрешили доступ.",
           );
         });

       // Настроим интервал для обработки видеопотока
       setInterval(processVideo, 100); // Обработка каждые 100 мс
     }

     // Обработка видеопотока для оценки глубины
     function processVideo() {
       if (typeof cv === "undefined") {
         console.error("OpenCV.js не загружен");
         return;
       }

       // Проверяем, готов ли видеопоток
       if (videoElement.readyState === videoElement.HAVE_ENOUGH_DATA) {
         // Создание Mat из видеопотока
         mat = cv.imread(videoElement);

         // Конвертация в градации серого (для приблизительной оценки глубины)
         let gray = new cv.Mat();
         cv.cvtColor(mat, gray, cv.COLOR_RGBA2GRAY);

         // Применение фильтра Гаусса для сглаживания
         let depthMap = new cv.Mat();
         cv.GaussianBlur(gray, depthMap, new cv.Size(5, 5), 1.5, 1.5);

         // Применение пороговой обработки
         thresholdMat = new cv.Mat();
         cv.threshold(depthMap, thresholdMat, 100, 255, cv.THRESH_BINARY);

         // Отображение результата на канвасе
         cv.imshow("depthCanvas", thresholdMat);

         // Очистка ресурсов
         mat.delete();
         gray.delete();
         depthMap.delete();
         thresholdMat.delete();
       }
     }
   </script>
 </body>
</html> -->
<!-- <!DOCTYPE html>

<html lang="en">
<head>
   <meta charset="UTF-8">
   <meta name="viewport" content="width=device-width, initial-scale=1.0">
   <title>3D Heatmap with Plane Detection</title>
   <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/0.153.0/three.min.js"></script>
   <script src="https://docs.opencv.org/4.x/opencv.js"></script>
   <style>
       body, html {
           margin: 0;
           overflow: hidden;
           background: black;
       }
       #canvas-container {
           position: relative;
           width: 100%;
           height: 100vh;
       }
       video, canvas {
           position: absolute;
           top: 0;
           left: 0;
       }
   </style>
</head>
<body>
   <div id="canvas-container">
       <video id="video" autoplay playsinline style="width: 100%; height: 100%;"></video>
       <canvas id="heatmap" style="width: 100%; height: 100%;"></canvas>
   </div>
   <script>
       const video = document.getElementById('video');
       const heatmapCanvas = document.getElementById('heatmap');
       const heatmapCtx = heatmapCanvas.getContext('2d');
       const container = document.getElementById('canvas-container');

       // Resize canvas to fit screen
       function resizeCanvas() {
           heatmapCanvas.width = container.offsetWidth;
           heatmapCanvas.height = container.offsetHeight;
       }
       resizeCanvas();
       window.addEventListener('resize', resizeCanvas);

       // Access device camera
       async function initCamera() {
           const stream = await navigator.mediaDevices.getUserMedia({ video: true });
           video.srcObject = stream;

           return new Promise(resolve => {
               video.onloadedmetadata = () => resolve();
           });
       }

       // Initialize OpenCV for plane detection and heatmap
       async function initOpenCV() {
           await initCamera();

           // Prepare OpenCV Mat objects
           const cap = new cv.VideoCapture(video);
           const frame = new cv.Mat(video.videoHeight, video.videoWidth, cv.CV_8UC4);
           const gray = new cv.Mat();
           const prevGray = new cv.Mat();
           let firstRun = true;

           // Heatmap parameters
           const heatmapData = Array.from({ length: heatmapCanvas.width * heatmapCanvas.height }, () => 0);

           // Render loop
           function renderHeatmap() {
               // Capture frame
               cap.read(frame);
               cv.cvtColor(frame, gray, cv.COLOR_RGBA2GRAY);

               if (!firstRun) {
                   // Optical flow (Lucas-Kanade method)
                   const flow = new cv.Mat();
                   cv.calcOpticalFlowFarneback(prevGray, gray, flow, 0.5, 3, 15, 3, 5, 1.2, 0);

                   // Process flow to create heatmap
                   for (let y = 0; y < flow.rows; y++) {
                       for (let x = 0; x < flow.cols; x++) {
                           const flowValue = flow.data[y * flow.cols + x];
                           const idx = y * heatmapCanvas.width + x;
                           heatmapData[idx] += Math.min(Math.abs(flowValue), 255);
                       }
                   }
                   flow.delete();
               } else {
                   firstRun = false;
               }

               // Update previous frame
               gray.copyTo(prevGray);

               // Render heatmap
               const imgData = heatmapCtx.getImageData(0, 0, heatmapCanvas.width, heatmapCanvas.height);
               for (let i = 0; i < heatmapData.length; i++) {
                   const colorValue = Math.min(heatmapData[i], 255);
                   imgData.data[i * 4] = colorValue;     // Red
                   imgData.data[i * 4 + 1] = 0;         // Green
                   imgData.data[i * 4 + 2] = 255 - colorValue; // Blue
                   imgData.data[i * 4 + 3] = 255;       // Alpha
               }
               heatmapCtx.putImageData(imgData, 0, 0);

               // Schedule next frame
               requestAnimationFrame(renderHeatmap);
           }

           renderHeatmap();
       }

       initOpenCV().catch(console.error);
   </script>
</body>
</html> -->



<html>
<!DOCTYPE html>
<html lang="en">
<head>
	<title>Show What Information is Being Sensed in the World</title>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
	<style>
		body, html {
			padding: 0;
			margin: 0;
			overflow: hidden;
			position: fixed;
			width: 100%;
			height: 100vh;
			-webkit-user-select: none;
			user-select: none;
		}
        #gui { position: absolute; top: 5%; right: 2px }
		#target {
			width: 100%;
			height: 100%;
			position: absolute;
		}
	</style>
	<link rel="stylesheet" href="../common.css"/>
	<script src="../libs/three.js"></script>
	<script src="../libs/three-gltf-loader.js"></script>
<!--    
        <script type="module" src="../../polyfill/XRPolyfill.js"></script>
        <script nomodule src="../../dist/webxr-polyfill.js"></script>
 -->        
        <script src="../../dist/webxr-polyfill.js"></script>
	<script src="../common.js"></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sensor Example</title>
</head>
<body>
<div id="target" />
<div onclick="hideMe(this)" id="description">
	<h2>Show World Knowledge</h2>
	<h5>(click to dismiss)</h5>
	<p>Render the anchors, including planes and face geometry, detected by the platform.
    </p>
</div>
<script>
    
    class WorldKnowledgeExample extends XRExampleBase {
        constructor(domElement){
            super(domElement, false, true, false, true)

            // A message at the bottom of the screen that shows whether a surface has been found
            this._messageEl = document.createElement('div')
            this.el.appendChild(this._messageEl)
            this._messageEl.style.position = 'absolute'
            this._messageEl.style.bottom = '10px'
            this._messageEl.style.left = '10px'
            this._messageEl.style.color = 'white'
            this._messageEl.style['font-size'] = '16px'

            this.anchorMap = new Map()
        }

        // Called during construction to allow the app to populate this.scene
        initializeScene() {
            // Add a box at the scene origin
            let box = new THREE.Mesh(
                new THREE.BoxBufferGeometry(0.1, 0.1, 0.1),
                new THREE.MeshPhongMaterial({color: '#DDFFDD'})
            )
            box.position.set(0, 0.05, 0)
            var axesHelper = AxesHelper( 0.2 );
            this.floorGroup.add( axesHelper );
            this.floorGroup.add(box)

            // Add a few lights
            this.ambientLight = new THREE.AmbientLight('#FFF', 0.2)
            this.scene.add(this.ambientLight);

            this.directionalLight = new THREE.DirectionalLight('#FFF', 0.6)
            this.directionalLight.position.set(0, 10, 0)
            this.scene.add(this.directionalLight)

			this.listenerSetup = false
        }

        // Called once per frame, before render, to give the app a chance to update this.scene
        updateScene(frame){
            if(frame.hasLightEstimate){
                // intensity is 1 for "normal"
                this.ambientLight.intensity = frame.lightEstimate * (2/10);
                this.directionalLight.intensity = frame.lightEstimate * (8/10);
            }
            if (!this.listenerSetup) {
                this.listenerSetup = true
                this.session.addEventListener(XRSession.NEW_WORLD_ANCHOR, this._handleNewWorldAnchor.bind(this))
                this.session.addEventListener(XRSession.UPDATE_WORLD_ANCHOR, this._handleUpdateWorldAnchor.bind(this))
                this.session.addEventListener(XRSession.REMOVE_WORLD_ANCHOR, this._handleRemoveWorldAnchor.bind(this))
                
            }
    <h1>Sensor Example</h1>
    <div id="output"></div>
    <script>
        const output = document.getElementById('output');

        function handleOrientation(event) {
            const alpha = event.alpha; // Rotation around Z-axis
            const beta = event.beta;   // Rotation around X-axis
            const gamma = event.gamma; // Rotation around Y-axis

            output.innerHTML = `
                <p>Alpha (Z): ${alpha.toFixed(2)}</p>
                <p>Beta (X): ${beta.toFixed(2)}</p>
                <p>Gamma (Y): ${gamma.toFixed(2)}</p>
            `;
}

        _handleUpdateWorldAnchor(event) {
            let anchor = event.detail

            if (anchor instanceof XRFaceAnchor) {
                if (anchor.geometry !== null) {
                    if (anchor.mesh) {
                        let currentVertexIndex = 0
                        var position = anchor.mesh.geometry.attributes.position;                    
                        for (let vertex of anchor.geometry.vertices) {
                            position.setXYZ(currentVertexIndex++, vertex.x, vertex.y, vertex.z)
                        }

                        //this.faceMesh.geometry.verticesNeedUpdate = true;
                        position.needsUpdate = true;                            
                    } else {
                        mesh = this.newMeshNode(anchor, '#999999', '#999900')
                        if (mesh) {
                            anchor.node.add(mesh)
                        }
                    }
                }
            } else if (anchor instanceof XRPlaneAnchor) {
                if (anchor.geometry !== null) {
                    if (anchor.mesh) {
                        if  ((anchor.mesh.extent[0] != anchor.extent[0]) || (anchor.mesh.extent[1] != anchor.extent[1])) {
                            // assume that any change to the plane will results in the extent changing at least a little
                            anchor.node.remove(anchor.mesh)

                            let mesh = this.newMeshNode(anchor, '#11FF11', '#009900')
                            if (mesh) {
                                mesh.extent = [anchor.extent[0], anchor.extent[1]]
                                anchor.node.add(mesh)
                            }
                        }
        if (typeof DeviceOrientationEvent !== 'undefined' && typeof DeviceOrientationEvent.requestPermission === 'function') {
            // iOS 13+ requires permission
            DeviceOrientationEvent.requestPermission()
                .then(permissionState => {
                    if (permissionState === 'granted') {
                        window.addEventListener('deviceorientation', handleOrientation);
} else {
                        mesh = this.newMeshNode(anchor, '#11FF11', '#009900')
                        if (mesh) {
                            mesh.extent = [anchor.extent[0], anchor.extent[1]]
                        }
                        output.innerHTML = '<p>Permission denied</p>';
}
                }
            }
		}

        _handleRemoveWorldAnchor(event) {
            let anchor = event.detail
            if (anchor.node !== null) {
                this.removeAnchoredNode(anchor.node);
            }                
                })
                .catch(console.error);
        } else if ('DeviceOrientationEvent' in window) {
            // For other devices
            window.addEventListener('deviceorientation', handleOrientation);
        } else {
            output.innerHTML = '<p>Device orientation not supported</p>';
}

        _handleNewWorldAnchor(event) {
            let anchor = event.detail
            let anchorGroup = new THREE.Group();
            var mesh = null;

	      	if (anchor instanceof XRFaceAnchor) {
                mesh = this.newMeshNode(anchor, '#999999', '#999900')
            } else if (anchor instanceof XRPlaneAnchor) {
                mesh = this.newMeshNode(anchor, '#11FF11', '#009900')
                if (mesh) {
                    mesh.extent = [anchor.extent[0], anchor.extent[1]]
                }
            }
            if (mesh) {
                anchorGroup.add(mesh)
            }
            var axesHelper = AxesHelper( 0.1 );
            anchorGroup.add( axesHelper );
            anchor.node = anchorGroup;
            this.addAnchoredNode(new XRAnchorOffset(anchor.uid), anchorGroup)
		}

        newMeshNode(anchor, edgeColor, polyColor) {
	      	if (anchor instanceof XRFaceAnchor || anchor instanceof XRPlaneAnchor) {
                if (anchor.geometry !== null) {
                    let mesh = new THREE.Group();

                    let vertexCount = anchor.geometry.vertexCount
                    let vertices = new Float32Array( vertexCount * 3 );
                    let currentVertexIndex = 0
                    for (let vertex of anchor.geometry.vertices) {
                        vertices[currentVertexIndex++] = vertex.x
                        vertices[currentVertexIndex++] = vertex.y
                        vertices[currentVertexIndex++] = vertex.z
                    }
                    
                    let triangleIndices = anchor.geometry.triangleIndices
                    let verticesBufferAttribute = new THREE.BufferAttribute( vertices, 3 )
                    verticesBufferAttribute.dynamic = true

                    let geometry = new THREE.BufferGeometry()
                    geometry.addAttribute( 'position', verticesBufferAttribute );
                    geometry.setIndex(triangleIndices)

                    // transparent mesh
                    var wireMaterial = new THREE.MeshPhongMaterial({color: edgeColor, wireframe: true})
                    var material = new THREE.MeshPhongMaterial({color: polyColor, transparent: true, opacity: 0.25})

                    mesh.add(new THREE.Mesh(geometry, material))
                    mesh.add(new THREE.Mesh(geometry, wireMaterial))

                    mesh.geometry = geometry;  // for later use

                    anchor.mesh = mesh;
                    return mesh
                }
            }
            return null;
		}
   }

    function AxesHelper( size ) {
        size = size || 1;

        var vertices = [
            0, 0, 0,	size, 0, 0,
            0, 0, 0,	0, size, 0,
            0, 0, 0,	0, 0, size
        ];

        var colors = [
            1, 0, 0,	1, 0.6, 0,
            0, 1, 0,	0.6, 1, 0,
            0, 0, 1,	0, 0.6, 1
        ];

        var geometry = new THREE.BufferGeometry();
        geometry.addAttribute( 'position', new THREE.Float32BufferAttribute( vertices, 3 ) );
        geometry.addAttribute( 'color', new THREE.Float32BufferAttribute( colors, 3 ) );

        var material = new THREE.LineBasicMaterial( { vertexColors: THREE.VertexColors } );

        return new THREE.LineSegments(geometry, material);
    }


    window.addEventListener('DOMContentLoaded', () => {
        setTimeout(() => {
            try {
                window.pageApp = new WorldKnowledgeExample(document.getElementById('target'))
            } catch(e) {
                console.error('page error', e)
            }
        }, 1000)
    })
</script>
    </script>
</body>
</html>
